# -*- coding: utf-8 -*-
"""이동장소추출.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18H8cmuR0J9JRHLYZCvZxbf5ZvpfICVzR

## 전처리
"""

import re
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# 인물 매핑 딕셔너리
person_mapping = {
    '임금': ['임금', '전하', '상감', '폐하', '과인'],
    '김류': ['김류', '영의정 김류', '영의정', '영상', '체찰사 김류', '체찰사'],
    '김상헌': ['김상헌', '예조판서 김상헌', '예판', '예판 김상헌 대감'],
    '최명길': ['최명길', '이조판서 최명길', '이판', '이판 최 대감'],
    '이시백': ['이시백', '수어사', '수어사 이시백'],
    '세자': ['세자', '동궁', '왕자'],
    '용골대': ['용골대', '청장 용골대'],
    '정명수': ['정명수', '천례 정명수', '정대인'],
    '서날쇠': ['서날쇠', '대장장이 서날쇠'],
    '칸': ['칸', '황제', '천자', '만승']
}

def preprocess_text(text):
    lines = text.split('\n')
    han_pattern = r'[\u4E00-\u9FFF\u3400-\u4DBF\u20000-\u2A6DF\u2A700-\u2B73F\u2B740-\u2B81F\u2B820-\u2CEAF\uF900-\uFAFF\u2F800-\u2FA1F]'
    lines = [re.sub(han_pattern, '', line) for line in lines]
    lines = [re.sub(r'[a-zA-Z]', '', line) for line in lines]
    text = '\n'.join(lines)
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'[^\w\s\.\,\!\?\-]', '', text)
    return text

def replace_names(text):
    for person, aliases in person_mapping.items():
        for alias in sorted(aliases, key=len, reverse=True):
            pattern = r'(?<!\<PS\>)(?:\S+\s+)?(?:' + '|'.join(re.escape(a) for a in aliases) + r')(?!\<\/PS\>)'
            text = re.sub(pattern, f'<PS>{person}</PS>', text)
    return text

def tag_entities(text, model_path):
    text = preprocess_text(text)  # 전처리 단계 추가
    text = replace_names(text)  # 인물 이름 매핑 추가
    model = AutoModelForTokenClassification.from_pretrained(model_path, local_files_only=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer)

    sentences = re.split(r'(?<=[.?!])\s+', text)
    tagged_sentences = []

    for sentence in sentences:
        results = ner_pipeline(sentence)
        merged_entities = []
        temp_entity, temp_tokens = "", []

        for result in results:
            entity, word = result['entity'], result['word']

            if entity.startswith("B-"):
                if temp_entity:
                    merged_entities.append((temp_entity, tokenizer.convert_tokens_to_string(temp_tokens)))
                temp_entity, temp_tokens = entity[2:], [word]
            elif entity.startswith("I-") and entity[2:] == temp_entity:
                temp_tokens.append(word)
            else:
                if temp_entity:
                    merged_entities.append((temp_entity, tokenizer.convert_tokens_to_string(temp_tokens)))
                temp_entity, temp_tokens = "", []
        if temp_entity:
            merged_entities.append((temp_entity, tokenizer.convert_tokens_to_string(temp_tokens)))

        for entity, word in merged_entities:
            sentence = re.sub(rf"<({entity})>.*?</\1>", word, sentence)  # Remove existing tags
            sentence = sentence.replace(word, f"<{entity}>{word}</{entity}>")
        tagged_sentences.append(sentence.strip())

    return '\n'.join(tagged_sentences)

def run_ner_tagging():
    input_file = "bookmarks.txt"
    output_file = "./tagged_sentences.txt"
    model_path = "C:/Users/teral/project-server/checkpoint-1008"

    with open(input_file, 'r', encoding='utf-8') as file:
        text = file.read()

    tagged_text = tag_entities(text, model_path)

    with open(output_file, 'w', encoding='utf-8') as file:
        file.write(tagged_text)

    print(f"Tagged text saved to {output_file}")

if __name__ == "__main__":
    run_ner_tagging()

"""## 인물별 이동 장소 추출"""

import pandas as pd
import re
import json

# 텍스트 파일에서 데이터를 읽고 DataFrame 생성
file_path = './tagged_sentences.txt'

# 텍스트 파일 읽기
with open(file_path, 'r', encoding='utf-8') as f:
    text = f.read()

# 텍스트를 문장 단위로 나누기
sentences = re.split(r'(?<=[.?!])\s+', text)

# JSON 파일에서 인물 목록 불러오기
json_path = './output.json'
with open(json_path, 'r', encoding='utf-8') as f:
    data = json.load(f)
    # "top_20_persons"에서 인물 이름 추출
    keywords = [person[0] for person in data.get("top_20_persons", [])]

# 문장 데이터로 DataFrame 생성
df = pd.DataFrame({'sent_form': sentences})

# "sent_form" 열에서 특정 단어들을 포함하는 문장 필터링
def filter_sentences_by_keywords(df, keywords):
    pattern = '|'.join(re.escape(keyword) for keyword in keywords)  # 키워드를 OR 조건으로 결합
    filtered_sentences = df['sent_form'][df['sent_form'].str.contains(pattern, na=False)]
    return '\n'.join(filtered_sentences.str.strip())  # 줄바꿈 및 양쪽 공백 제거

# 특정 단어들을 포함하는 문장 필터링
filtered_sentences = filter_sentences_by_keywords(df, keywords)

# 결과 출력
print(filtered_sentences)

# 결과를 파일로 저장 (옵션)
output_path = './filtered_sentences.txt'
with open(output_path, 'w', encoding='utf-8') as f:
    f.write(filtered_sentences)

"""## 인물별 이동여부 & 장소"""

import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 파일 경로 설정
file_path = "./filtered_sentences.txt"

# 텍스트 파일에서 문장 읽기
with open(file_path, 'r', encoding='utf-8') as file:
    example_sentences = [line.strip() for line in file.readlines() if line.strip()]

# 이동 경로를 저장할 딕셔너리
movement_dict = {}

# 모델 및 토크나이저 로드
model_path = 'C:/Users/teral/project-server/checkpoint-1500'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

def predict_movement(sentence):
    persons, locations = extract_entities(sentence)
    predictions = []
    if not persons or not locations:
        return predictions
    for person in persons:
        for location in locations:
            # 엔티티 마킹
            marked_sentence = sentence
            marked_sentence = marked_sentence.replace(f'<PS>{person}</PS>', f'[E1] {person} [/E1]')
            marked_sentence = marked_sentence.replace(f'<LC>{location}</LC>', f'[E2] {location} [/E2]')
            marked_sentence = re.sub(r'</?PS>', '', marked_sentence)
            marked_sentence = re.sub(r'</?LC>', '', marked_sentence)
            # 토큰화
            inputs = tokenizer(
                marked_sentence,
                return_tensors='pt',
                padding='max_length',
                truncation=True,
                max_length=128
            )
            # 모델이 사용 중인 장치로 데이터 이동
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            inputs = {key: value.to(device) for key, value in inputs.items()}
            model.to(device)
            # 모델 예측
            outputs = model(**inputs)
            logits = outputs.logits
            pred = torch.argmax(logits, dim=1).item()
            if pred == 1:
                predictions.append((person, location))
    return predictions

def extract_entities(sentence):
    persons = re.findall(r'<PS>(.*?)</PS>', sentence)
    locations = re.findall(r'<LC>(.*?)</LC>', sentence)
    return persons, locations

# 각 문장에 대해 이동 여부 예측
for sentence in example_sentences:
    predictions = predict_movement(sentence)
    for person, location in predictions:
        if person not in movement_dict:
            movement_dict[person] = []  # 새로운 인물 추가
        if location not in movement_dict[person]:
            movement_dict[person].append(location)  # 장소 추가

# 인물별 이동 경로 출력
print("\n=== 인물별 이동 경로 ===")
for person, locations in movement_dict.items():
    location_list = ', '.join(locations)
    print(f"{person}: {location_list}")

# JSON 파일로 저장
json_file_path = "./movement_data.json"
with open(json_file_path, 'w', encoding='utf-8') as json_file:
    json.dump(movement_dict, json_file, ensure_ascii=False, indent=4)

print(f"\nJSON 파일로 저장 완료: {json_file_path}")

