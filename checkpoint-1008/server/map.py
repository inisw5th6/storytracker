# -*- coding: utf-8 -*-
"""map.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wSrit7mEH4T6NV_E-l75UGScpuLx-n5M
"""

import re
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# 인물 매핑 딕셔너리
person_mapping = {
    '임금': ['임금', '전하', '상감', '폐하', '과인'],
    '김류': ['김류', '영의정 김류', '영의정', '영상', '체찰사 김류', '체찰사'],
    '김상헌': ['김상헌', '예조판서 김상헌', '예판', '예판 김상헌 대감'],
    '최명길': ['최명길', '이조판서 최명길', '이판', '이판 최 대감'],
    '이시백': ['이시백', '수어사', '수어사 이시백'],
    '세자': ['세자', '동궁', '왕자'],
    '용골대': ['용골대', '청장 용골대'],
    '정명수': ['정명수', '천례 정명수', '정대인'],
    '서날쇠': ['서날쇠', '대장장이 서날쇠'],
    '칸': ['칸', '황제', '천자', '만승']
}

def preprocess_text(text):
    lines = text.split('\n')
    han_pattern = r'[\u4E00-\u9FFF\u3400-\u4DBF\u20000-\u2A6DF\u2A700-\u2B73F\u2B740-\u2B81F\u2B820-\u2CEAF\uF900-\uFAFF\u2F800-\u2FA1F]'
    lines = [re.sub(han_pattern, '', line) for line in lines]
    lines = [re.sub(r'[a-zA-Z]', '', line) for line in lines]
    text = '\n'.join(lines)
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'[^\w\s\.\,\!\?\-]', '', text)
    return text

def replace_names(text):
    for person, aliases in person_mapping.items():
        for alias in sorted(aliases, key=len, reverse=True):
            pattern = r'(?<!\<PS\>)(?:\S+\s+)?(?:' + '|'.join(re.escape(a) for a in aliases) + r')(?!\<\/PS\>)'
            text = re.sub(pattern, f'<PS>{person}</PS>', text)
    return text

def tag_entities(text, model_path):
    text = preprocess_text(text)  # 전처리 단계 추가
    text = replace_names(text)  # 인물 이름 매핑 추가
    model = AutoModelForTokenClassification.from_pretrained(model_path, local_files_only=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer)

    sentences = re.split(r'(?<=[.?!])\s+', text)
    tagged_sentences = []

    for sentence in sentences:
        results = ner_pipeline(sentence)
        merged_entities = []
        temp_entity, temp_tokens = "", []

        for result in results:
            entity, word = result['entity'], result['word']

            if entity.startswith("B-"):
                if temp_entity:
                    merged_entities.append((temp_entity, tokenizer.convert_tokens_to_string(temp_tokens)))
                temp_entity, temp_tokens = entity[2:], [word]
            elif entity.startswith("I-") and entity[2:] == temp_entity:
                temp_tokens.append(word)
            else:
                if temp_entity:
                    merged_entities.append((temp_entity, tokenizer.convert_tokens_to_string(temp_tokens)))
                temp_entity, temp_tokens = "", []
        if temp_entity:
            merged_entities.append((temp_entity, tokenizer.convert_tokens_to_string(temp_tokens)))

        for entity, word in merged_entities:
            sentence = re.sub(rf"<({entity})>.*?</\1>", word, sentence)  # Remove existing tags
            sentence = sentence.replace(word, f"<{entity}>{word}</{entity}>")
        tagged_sentences.append(sentence.strip())

    return '\n'.join(tagged_sentences)

def run_ner_tagging():
    input_file = "C:/Users/teral/project-server/bookmarks.txt"
    output_file = "./tagged_sentences.json"
    model_path = "C:/Users/teral/project-server/checkpoint-1008"

    with open(input_file, 'r', encoding='utf-8') as file:
        text = file.read()

    tagged_text = tag_entities(text, model_path)

    with open(output_file, 'w', encoding='utf-8') as file:
        file.write(tagged_text)

    print(f"Tagged text saved to {output_file}")

if __name__ == "__main__":
    run_ner_tagging()

import pandas as pd
import re
import json
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 텍스트 파일에서 데이터를 읽고 DataFrame 생성

file_path =  "./tagged_sentences.json"

# 텍스트 파일 읽기
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

# 텍스트를 문장 단위로 나누기
sentences = re.split(r'(?<=[.?!])\s+', text)

# JSON 파일에서 인물 목록 불러오기
json_path = 'C:/Users/teral/project-server/output.json'
with open(json_path, 'r', encoding='utf-8') as f:
    data = json.load(f)
    # "top_20_persons"에서 인물 이름 추출
    keywords = [person[0] for person in data.get("top_20_persons", [])]

# 문장 데이터로 DataFrame 생성
df = pd.DataFrame({'sent_form': sentences})

# 특정 단어들을 포함하는 문장 필터링 함수 정의
def filter_sentences_by_keywords(df, keywords):
    pattern = '|'.join(re.escape(keyword) for keyword in keywords)  # 키워드를 OR 조건으로 결합
    filtered_sentences = df['sent_form'][df['sent_form'].str.contains(pattern, na=False)]
    return '\n'.join(filtered_sentences.str.strip())  # 줄바꿈 및 양쪽 공백 제거

# 이동 여부 예측을 위한 모델 및 토크나이저 로드
model_path = 'C:/Users/teral/project-server/checkpoint-2990'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

def predict_movement(sentence):
    persons, locations = extract_entities(sentence)
    predictions = []
    if not persons or not locations:
        return predictions
    for person in persons:
        for location in locations:
            marked_sentence = sentence
            marked_sentence = marked_sentence.replace(f'<PS>{person}</PS>', f'[E1] {person} [/E1]')
            marked_sentence = marked_sentence.replace(f'<LC>{location}</LC>', f'[E2] {location} [/E2]')
            marked_sentence = re.sub(r'</?PS>', '', marked_sentence)
            marked_sentence = re.sub(r'</?LC>', '', marked_sentence)
            inputs = tokenizer(
                marked_sentence,
                return_tensors='pt',
                padding='max_length',
                truncation=True,
                max_length=128
            )
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            inputs = {key: value.to(device) for key, value in inputs.items()}
            model.to(device)
            outputs = model(**inputs)
            logits = outputs.logits
            pred = torch.argmax(logits, dim=1).item()
            if pred == 1:
                predictions.append((person, location))
    return predictions

def extract_entities(sentence):
    persons = re.findall(r'<PS>(.*?)</PS>', sentence)
    locations = re.findall(r'<LC>(.*?)</LC>', sentence)
    return persons, locations

def extract_location_sentences_with_movement(sentences):
    location_sentences = []
    pattern = r'(<LC>(.*?)</LC>.*?\.)'

    for sentence in sentences:
        predictions = predict_movement(sentence)
        if predictions:
            match = re.search(pattern, sentence)
            if match:
                location = match.group(2)  # LC 태그 내부의 장소
                clean_sentence = re.sub(r'<.*?>', '', sentence)  # 태그 제거
                location_sentences.append({location: clean_sentence})

    return location_sentences

# 이동 여부가 있는 문장만 추출
location_sentences = extract_location_sentences_with_movement(sentences)

# 장소별 문장 결과 출력
for entry in location_sentences:
    for location, sentence in entry.items():
        print(f"{location}: {sentence}")

# 결과를 파일로 저장 (옵션)
output_path = './location_sentences.json'
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(location_sentences, f, ensure_ascii=False, indent=4)

import json
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from geopy.geocoders import Nominatim
from geopy.distance import great_circle

output_path = './location_sentences.json'
model_path = "C:/Users/teral/project-server/checkpoint-1008"

# 모델과 토크나이저 로드 함수
def load_model(model_path):
    model = AutoModel.from_pretrained(model_path, local_files_only=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
    return model, tokenizer

# 문장 임베딩 추출 함수
def get_sentence_embedding(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    embedding = outputs.last_hidden_state[:, 0, :].numpy()
    return embedding

# 지리적 근접성 계산 함수
def calculate_geographical_proximity(candidate_location, previous_locations):
    if not previous_locations:
        return 0  # 이전 위치가 없으면 근접성 0
    distances = []
    for prev_loc in previous_locations:
        prev_point = (prev_loc['latitude'], prev_loc['longitude'])
        cand_point = (candidate_location.latitude, candidate_location.longitude)
        distance = great_circle(prev_point, cand_point).kilometers
        distances.append(distance)
    avg_distance = sum(distances) / len(distances)
    # 거리가 가까울수록 근접성 점수가 높아지도록 (역수 사용)
    proximity_score = 1 / (avg_distance + 1)
    return proximity_score

def select_locations(json_path, n_geo_locations=5, n_candidate_locations=5, alpha=0.7, model_path=None):
    if model_path is None:
        raise ValueError("모델 경로를 지정해야 합니다.")

    # 모델 로드
    model, tokenizer = load_model(model_path)
    geolocator = Nominatim(user_agent="geo_example")

    # JSON 파일 로드
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    previous_locations = []
    final_dict = {}

    # data는 [{"장소명":"문장"}, {"장소명":"문장"}, ...] 형태라고 가정
    for item in data:
        for place_name, sentence in item.items():
            # 해당 장소명에 대한 문장 임베딩
            sentence_embedding = get_sentence_embedding(sentence, model, tokenizer)

            # 지오코딩 후보 위치 가져오기
            locations = geolocator.geocode(place_name, exactly_one=False, limit=n_candidate_locations)

            if locations:
                candidate_scores = []
                for loc in locations:
                    # 후보 주소와 지명을 합쳐서 임베딩
                    loc_description = loc.address + ' ' + place_name
                    loc_embedding = get_sentence_embedding(loc_description, model, tokenizer)

                    # 코사인 유사도
                    similarity = cosine_similarity(sentence_embedding, loc_embedding)[0][0]

                    # 지리적 근접성
                    recent_previous_locations = previous_locations[-n_geo_locations:]
                    proximity_score = calculate_geographical_proximity(loc, recent_previous_locations)

                    # 종합 스코어
                    total_score = (similarity * alpha) + (proximity_score * (1 - alpha))
                    candidate_scores.append((total_score, loc))

                # 가장 스코어 높은 후보 선택
                best_match = max(candidate_scores, key=lambda x: x[0])
                selected_location = best_match[1]

                # 선택한 장소 정보 저장
                location_info = {
                    'name': place_name,
                    'latitude': selected_location.latitude,
                    'longitude': selected_location.longitude,
                    'address': selected_location.address
                }
                previous_locations.append(location_info)

                # 최종 딕셔너리에 추가
                final_dict[place_name] = (selected_location.latitude, selected_location.longitude)

                # previous_locations 길이 제한
                if len(previous_locations) > n_geo_locations:
                    previous_locations = previous_locations[-n_geo_locations:]
            else:
                print(f"{place_name}의 좌표를 찾을 수 없습니다.")

    return final_dict

# 함수 호출 예시
selected_dict = select_locations(
    json_path=output_path,
    n_geo_locations=5,
    n_candidate_locations=5,
    alpha=0.7,
    model_path=model_path
)

print("\n최종 선택된 위치들:")
print(selected_dict)


# 최종 결과를 JSON 파일에 저장
output_file = './selected_locations.json'  # 저장할 파일 경로

with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(selected_dict, f, ensure_ascii=False, indent=4)

print(f"\n최종 선택된 위치들이 {output_file}에 저장되었습니다.")

