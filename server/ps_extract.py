# -*- coding: utf-8 -*-
"""PS추출.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1udbJqwzSolnZ4gfgCpFStyZOIpbbkhac
"""

import re
import json
from collections import defaultdict
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import torch

def preprocess_text(text):
    han_pattern = r'[\u4E00-\u9FFF\u3400-\u4DBF\u20000-\u2A6DF\u2A700-\u2B73F\u2B740-\u2B81F\u2B820-\u2CEAF\uF900-\uFAFF\u2F800-\u2FA1F]'
    text = re.sub(han_pattern, '', text)
    text = re.sub(r'[a-zA-Z]', '', text)
    return re.sub(r'[^\w\s\.\,\!\?\-]', '', text).strip()

def process_sentence(sentence, ner_pipeline, tokenizer):
    results = ner_pipeline(sentence)
    persons = []
    current_person = []
    last_end = -1
    for result in results:
        if result['entity'] in ['B-PS', 'I-PS']:
            if not current_person or result['start'] == last_end + 1 or result['word'].startswith('##'):
                current_person.append(result['word'])
            else:
                full_name = tokenizer.decode(tokenizer.convert_tokens_to_ids(current_person))
                persons.append(full_name)
                current_person = [result['word']]
            last_end = result['end']
        else:
            if current_person:
                full_name = tokenizer.decode(tokenizer.convert_tokens_to_ids(current_person))
                persons.append(full_name)
                current_person = []
    if current_person:
        full_name = tokenizer.decode(tokenizer.convert_tokens_to_ids(current_person))
        persons.append(full_name)
    return persons

def count_co_occurrences(all_persons):
    co_occurrence = defaultdict(lambda: defaultdict(int))
    person_frequency = defaultdict(int)
    for persons in all_persons:
        for i, person in enumerate(persons):
            person_frequency[person] += 1
            for other_person in persons[i+1:]:
                co_occurrence[person][other_person] += 1
                co_occurrence[other_person][person] += 1
    return co_occurrence, person_frequency

def calculate_jaccard_similarity(co_occurrences, top_20_names):
    jaccard_similarity = {}
    for person1 in top_20_names:
        jaccard_similarity[person1] = {}
        for person2 in top_20_names:
            if person1 != person2:
                co_occur = co_occurrences[person1][person2]
                total = co_occur + (sum(co_occurrences[person1].values()) - co_occur) + (sum(co_occurrences[person2].values()) - co_occur)
                jaccard = co_occur / total if total > 0 else 0
                jaccard_similarity[person1][person2] = jaccard
    return jaccard_similarity

def save_to_json(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def analyze_text_from_file_and_save_json(input_file_path, model_path, output_file_path):
    # GPU 또는 CPU 설정
    device = 0 if torch.cuda.is_available() else -1

    # 모델 경로 로드 (로컬 디렉토리 사용)
    model = AutoModelForTokenClassification.from_pretrained(model_path, local_files_only=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, device=device)

    # 텍스트 파일 읽기
    with open(input_file_path, 'r', encoding='utf-8') as f:
        text_list = f.readlines()

    all_persons = []
    for text in text_list:
        preprocessed_text = preprocess_text(text)
        sentences = re.split(r'(?<=[.!?])\s+', preprocessed_text)
        for sentence in sentences:
            persons = process_sentence(sentence, ner_pipeline, tokenizer)
            all_persons.append(persons)

    co_occurrences, person_frequency = count_co_occurrences(all_persons)

    # 상위 20명의 인물 추출
    top_20_persons = sorted(person_frequency.items(), key=lambda x: x[1], reverse=True)[:20]
    top_20_names = [person for person, _ in top_20_persons]

    # 자카드 유사도 계산
    jaccard_similarity = calculate_jaccard_similarity(co_occurrences, top_20_names)

    # 결과 JSON으로 저장
    data = {
        "top_20_persons": top_20_persons,
        "jaccard_similarity": jaccard_similarity
    }
    save_to_json(data, output_file_path)

# 호출 함수
def run_analysis():
    input_file = "C:/Users/teral/project-server/bookmarks.txt"
    model_path =  "C:/Users/teral/project-server/checkpoint-1008"  # 고정된 모델 경로
    output_file = "./output.json"  # 현재 디렉토리에 저장

    print(f"모델 경로: {model_path}")
    print(f"결과 저장 경로: {output_file}")

    analyze_text_from_file_and_save_json(input_file, model_path, output_file)

if __name__ == "__main__":
    run_analysis()

